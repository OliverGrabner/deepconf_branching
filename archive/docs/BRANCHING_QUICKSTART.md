# Branching Self-Consistency - Quick Start

## What is it?

A smarter version of self-consistency that:
- Starts with 8 traces
- Branches high-confidence traces during generation
- Reaches 32 traces by 75% of generation
- Stops exploring new ideas late in reasoning (when it's just verification)

## Quick Start (3 Steps)

### 1. Bootstrap Historical Stats (Run Once)

```bash
python scripts/compute_historical_stats.py \
    --model deepseek-ai/DeepSeek-R1-0528-Qwen3-8B \
    --num_samples 2
```

**What it does**: Generates 2 traces per question to learn average token counts
**Output**: `historical_stats/aime25_token_stats_latest.json`
**Time**: ~10-20 minutes for full AIME25

### 2. Run Branching SC

```bash
python scripts/run_branching_sc_aime25.py \
    --model deepseek-ai/DeepSeek-R1-0528-Qwen3-8B \
    --start_traces 8 \
    --max_traces 32 \
    --historical_stats historical_stats/aime25_token_stats_latest.json
```

**What it does**: Runs branching SC on all AIME25 questions
**Output**: Results in `outputs_sc/branching_sc_aime25_*.json` and `.csv`

### 3. Compare to Baseline

Run traditional SC for comparison:

```bash
python scripts/run_traditional_sc_aime25.py \
    --model deepseek-ai/DeepSeek-R1-0528-Qwen3-8B \
    --num_traces 32
```

## Quick Test (Single Question)

Test on one question first:

```bash
python scripts/test_branching_single_question.py \
    --qid 0 \
    --start_traces 4 \
    --max_traces 8 \
    --n_iterations 5
```

## What Gets Saved

### Detailed Results
`outputs_sc/branching_sc_aime25_detailed_YYYYMMDD_HHMMSS.json`
- All traces with full text
- Branch genealogy (who spawned who)
- Timing and token counts

### Summary CSV
`outputs_sc/branching_sc_aime25_summary_YYYYMMDD_HHMMSS.csv`
- One row per question
- Correctness, traces, branches, tokens

### Statistics
`outputs_sc/branching_sc_aime25_stats_YYYYMMDD_HHMMSS.json`
- Accuracy per dataset
- Average branches per question
- Token efficiency

## Key Parameters

| Parameter | Default | Meaning |
|-----------|---------|---------|
| `start_traces` | 8 | Initial traces |
| `max_traces` | 32 | Final traces after branching |
| `selected_percent` | 0.60 | Top 60% eligible for branching |
| `n_iterations` | 10 | How many branching checkpoints |
| `branch_goal` | 0.75 | Stop branching at 75% of generation |

## How It Works

```
Iteration 0 (0 tokens):    [8 traces]
Iteration 1 (600 tokens):  [8 traces] → branch top 60% → [11 traces]
Iteration 2 (1200 tokens): [11 traces] → branch top 60% → [14 traces]
...
Iteration 10 (6000 tokens): [32 traces] → STOP BRANCHING
Continue to completion:     [32 traces] → generate to end
Final voting:               Majority vote on 32 answers
```

## Expected Results

Compare to traditional SC with same trace budget (32):

**Potential improvements:**
- Similar or better accuracy
- Better token efficiency (share prefixes)
- More focused exploration early in reasoning

**Analysis opportunities:**
- Track which traces contributed to correct answer
- See when/where reasoning paths diverge
- Identify high-value branches

## Troubleshooting

### "No such file: historical_stats/..."

Run `compute_historical_stats.py` first to generate the stats file.

### "CUDA out of memory"

Reduce `max_traces` or `tensor_parallel_size`:

```bash
python scripts/run_branching_sc_aime25.py \
    --max_traces 16 \
    --tensor_parallel_size 2 \
    ...
```

### Branching creates fewer traces than expected

This is OK! The algorithm uses `ceil()` so it might hit `max_traces` before all iterations complete.

### Want to run on subset

```bash
python scripts/run_branching_sc_aime25.py \
    --dataset AIME2025-I \
    --start_idx 0 \
    --end_idx 5 \
    ...
```

## Next Steps

1. **Run experiments**: Compare branching SC vs traditional SC
2. **Analyze genealogy**: See which branches helped
3. **Tune parameters**: Try different `start_traces`, `selected_percent`, etc.
4. **Visualize**: Plot branch trees, confidence evolution

See [BRANCHING_SC.md](docs/BRANCHING_SC.md) for full documentation.

## File Structure

```
deepconf_branching/
├── deepconf/
│   ├── branching.py              # BranchingManager implementation
│   ├── wrapper.py                # DeepThinkLLM with branching mode
│   └── outputs.py                # Extended with genealogy fields
├── scripts/
│   ├── compute_historical_stats.py          # Step 1: Bootstrap
│   ├── run_branching_sc_aime25.py           # Step 2: Run experiments
│   ├── test_branching_single_question.py    # Quick test
│   └── run_traditional_sc_aime25.py         # Baseline comparison
├── historical_stats/
│   └── aime25_token_stats_latest.json       # Generated by step 1
├── outputs_sc/
│   └── branching_sc_aime25_*.{json,csv}     # Results from step 2
└── docs/
    └── BRANCHING_SC.md                       # Full documentation
```
