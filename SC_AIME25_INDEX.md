# Traditional Self-Consistency on AIME 2025 - Complete Index

## Quick Links

- **Want to run it now?** ‚Üí [QUICK_START_SC.md](QUICK_START_SC.md)
- **Need detailed docs?** ‚Üí [README_SC_AIME25.md](README_SC_AIME25.md)
- **Want overview?** ‚Üí [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)

## All Files Created

### üìã Documentation (Start Here!)

| File | Purpose | Who It's For |
|------|---------|--------------|
| [QUICK_START_SC.md](QUICK_START_SC.md) | TL;DR commands and examples | Everyone (read this first!) |
| [README_SC_AIME25.md](README_SC_AIME25.md) | Comprehensive documentation | Detailed reference |
| [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md) | Technical overview | Understanding the implementation |
| [SC_AIME25_INDEX.md](SC_AIME25_INDEX.md) | This file - navigation guide | Finding what you need |

### üîß Core Implementation

| File | Lines | Purpose |
|------|-------|---------|
| [run_traditional_sc_aime25.py](run_traditional_sc_aime25.py) | ~550 | Main script - runs SC on AIME25 |
| [requirements_sc.txt](requirements_sc.txt) | ~15 | Python dependencies |

### üß™ Testing & Utilities

| File | Lines | Purpose |
|------|-------|---------|
| [test_sc_single_question.py](test_sc_single_question.py) | ~120 | Quick test on 1 question |
| [analyze_sc_results.py](analyze_sc_results.py) | ~300 | Deep analysis of results |
| [visualize_sc_results.py](visualize_sc_results.py) | ~350 | ASCII visualizations |
| [run_sc_experiments.sh](run_sc_experiments.sh) | ~70 | Automated experiments |

### üìä Output Files (Generated When You Run)

Located in `outputs_sc/`:

| File | Format | Contains |
|------|--------|----------|
| `traditional_sc_aime25_detailed_*.json` | JSON | Complete trace data |
| `traditional_sc_aime25_summary_*.csv` | CSV | Spreadsheet-friendly summary |
| `traditional_sc_aime25_stats_*.json` | JSON | Aggregate statistics |
| `*_analysis.csv` | CSV | Generated by analyze script |

## Usage Workflows

### Workflow 1: First Time User (10 minutes)

```bash
# 1. Install dependencies (2 min)
pip install -r requirements_sc.txt

# 2. Quick test (5 min)
python test_sc_single_question.py

# 3. Read the output and understand what happened
#    See QUICK_START_SC.md for interpretation
```

### Workflow 2: Run Full Experiment (30-40 minutes)

```bash
# 1. Run on both datasets with 64 traces
python run_traditional_sc_aime25.py --num_traces 64

# 2. While it runs, read IMPLEMENTATION_SUMMARY.md
#    to understand what's happening

# 3. When complete, analyze results
python analyze_sc_results.py outputs_sc/traditional_sc_aime25_detailed_*.json

# 4. Visualize results
python visualize_sc_results.py outputs_sc/traditional_sc_aime25_detailed_*.json
```

### Workflow 3: Research Experiment (Hours)

```bash
# 1. Run automated experiments with different configs
./run_sc_experiments.sh

# 2. Compare results across different trace budgets
python analyze_sc_results.py outputs_sc/traditional_sc_aime25_detailed_*_8traces.json
python analyze_sc_results.py outputs_sc/traditional_sc_aime25_detailed_*_64traces.json
python analyze_sc_results.py outputs_sc/traditional_sc_aime25_detailed_*_128traces.json

# 3. Generate comparison plots and tables
# (Use pandas/matplotlib for custom analysis)
```

### Workflow 4: Single Dataset Only (15-20 minutes)

```bash
# Just AIME25-I
python run_traditional_sc_aime25.py --dataset AIME2025-I --num_traces 64

# Just AIME25-II
python run_traditional_sc_aime25.py --dataset AIME2025-II --num_traces 64
```

## Command Reference by Use Case

### Quick Testing

```bash
# Fastest test (1 question, 8 traces, ~2 min)
python test_sc_single_question.py

# Fast partial test (5 questions, 16 traces, ~5 min)
python run_traditional_sc_aime25.py --end_idx 5 --num_traces 16

# Medium partial test (10 questions, 32 traces, ~10 min)
python run_traditional_sc_aime25.py --end_idx 10 --num_traces 32
```

### Standard Experiments

```bash
# Standard SC (64 traces, both datasets, ~30-40 min)
python run_traditional_sc_aime25.py --num_traces 64

# Low budget (16 traces, ~10 min)
python run_traditional_sc_aime25.py --num_traces 16

# High budget (128 traces, ~60-80 min)
python run_traditional_sc_aime25.py --num_traces 128
```

### Analysis

```bash
# Detailed analysis
python analyze_sc_results.py outputs_sc/traditional_sc_aime25_detailed_*.json

# Visualizations
python visualize_sc_results.py outputs_sc/traditional_sc_aime25_detailed_*.json

# View CSV in terminal
cat outputs_sc/traditional_sc_aime25_summary_*.csv | column -t -s, | less -S

# View stats JSON
python -m json.tool outputs_sc/traditional_sc_aime25_stats_*.json
```

## Understanding Output

### What the Metrics Mean

| Metric | Meaning | Good Value |
|--------|---------|------------|
| **Accuracy** | % questions correct with voting | 40-70% for AIME |
| **Individual Trace Accuracy** | % correct in single traces | 30-50% typically |
| **SC Improvement** | Voting accuracy - Individual accuracy | +10-20% |
| **Consensus** | % votes for winning answer | >80% is high |
| **Valid Traces** | Traces with extracted answers | Should be ~100% |
| **Throughput** | Tokens generated per second | 1500-2500 tokens/s |

### Interpreting a Single Question Result

```
Q0: ‚úì                                    ‚Üê Correct!
  Ground Truth: 42                       ‚Üê Right answer
  Voted Answer: 42                       ‚Üê What we predicted
  Valid Traces: 64/64                    ‚Üê All traces extracted answer
  Individual Accuracy: 78.1%             ‚Üê 50/64 traces were correct
  Vote Distribution: {'42': 50, '43': 10, '41': 4}  ‚Üê Vote breakdown
  Tokens: 98,432 (1,538.0 avg)          ‚Üê Computational cost
  Time: 45.32s                           ‚Üê Wall time
```

**Key Insights:**
- High individual accuracy (78%) + high consensus ‚Üí confident correct answer
- Vote distribution shows 50/64 agreed on correct answer
- 10 traces said 43 (close!), 4 said 41 (also close!)

### Reading the Final Summary

```
Overall Results (AIME25-I + AIME25-II):
  Total Questions: 30                    ‚Üê Number of problems
  Total Correct: 17/30 (56.7%)          ‚Üê Main result!
  Total Tokens: 2,432,801                ‚Üê Computational cost
  Total Time: 1224.6s (20.4 minutes)    ‚Üê Wall time
  Overall Throughput: 1,986.8 tokens/sec ‚Üê Generation speed
```

**56.7% accuracy on AIME is excellent!** (AIME is very hard)

## Troubleshooting Guide

| Problem | Solution | File Reference |
|---------|----------|----------------|
| Don't know where to start | Read QUICK_START_SC.md | [Link](QUICK_START_SC.md) |
| Installation issues | Check requirements_sc.txt | [Link](requirements_sc.txt) |
| Out of memory | Reduce --num_traces or --max_tokens | [README](README_SC_AIME25.md#troubleshooting) |
| Slow generation | Check GPU setup and tensor_parallel_size | [README](README_SC_AIME25.md#troubleshooting) |
| Understanding results | Use analyze_sc_results.py | [Link](analyze_sc_results.py) |
| Need visualizations | Use visualize_sc_results.py | [Link](visualize_sc_results.py) |

## FAQ

### Q: How long does it take?

**A:**
- Quick test: 2-3 minutes
- Full experiment (64 traces, both datasets): 30-40 minutes
- High budget (128 traces): 60-80 minutes

### Q: How much GPU memory needed?

**A:** With 4x RTX 5000 Ada (128GB total), you should be fine. If OOM:
- Reduce `--num_traces` (try 32 or 16)
- Reduce `--max_tokens` (try 65000)

### Q: What accuracy should I expect?

**A:** AIME is extremely hard. Typical results:
- Individual traces: 30-50% correct
- After voting: 40-70% correct
- Improvement: +10-20%

### Q: How do I compare different configurations?

**A:** Run multiple experiments and compare the `*_stats.json` files:
```bash
python run_traditional_sc_aime25.py --num_traces 16
python run_traditional_sc_aime25.py --num_traces 64
python run_traditional_sc_aime25.py --num_traces 128

# Then compare the accuracy in each *_stats.json
```

### Q: Can I use a different model?

**A:** Yes! Change the `--model` argument:
```bash
python run_traditional_sc_aime25.py \
    --model "meta-llama/Llama-3.1-70B-Instruct" \
    --model_type gpt \
    --num_traces 64
```

### Q: What's the difference between this and DeepConf's other voting methods?

**A:** This implements **only traditional SC (simple majority voting)**. DeepConf supports 7 methods including confidence-weighted voting. This is the baseline to compare against.

## Integration with DeepConf Framework

### What We Use from DeepConf

```python
from deepconf import (
    DeepThinkLLM,      # vLLM wrapper
    prepare_prompt,     # Prompt formatting
    equal_func          # Math-aware comparison
)
```

### What We Don't Use

- Multiple voting methods (we implement our own)
- Confidence weighting (not part of traditional SC)
- Online mode (we use offline batch generation)

### Why Separate Implementation?

Traditional SC is **specifically** simple majority voting. We want a clean baseline implementation for comparison.

## Next Steps After Running

### 1. Understand Your Results

```bash
# Read the analysis
python analyze_sc_results.py outputs_sc/traditional_sc_aime25_detailed_*.json

# Visualize patterns
python visualize_sc_results.py outputs_sc/traditional_sc_aime25_detailed_*.json
```

### 2. Try Different Configurations

```bash
# Different trace budgets
./run_sc_experiments.sh

# Different temperatures
python run_traditional_sc_aime25.py --num_traces 64 --temperature 0.8
python run_traditional_sc_aime25.py --num_traces 64 --temperature 1.2
```

### 3. Compare with Advanced Methods

```bash
# Run DeepConf's 7 voting methods
python examples/example_offline.py --dataset your_data.jsonl --qid 0 --budget 64
```

### 4. Deep Dive Analysis

Use pandas/matplotlib for custom analysis:
```python
import json
import pandas as pd

with open('outputs_sc/traditional_sc_aime25_detailed_*.json') as f:
    data = json.load(f)

df = pd.read_csv('outputs_sc/traditional_sc_aime25_summary_*.csv')

# Your custom analysis here
```

## Citation

If you use this implementation, cite:

```bibtex
@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}
```

## Support

- **Implementation questions**: Check code comments in .py files
- **Usage questions**: See [QUICK_START_SC.md](QUICK_START_SC.md)
- **Technical details**: See [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)
- **Troubleshooting**: See [README_SC_AIME25.md](README_SC_AIME25.md)

---

**Ready to start?**

```bash
pip install -r requirements_sc.txt
python test_sc_single_question.py
```

Then read [QUICK_START_SC.md](QUICK_START_SC.md) to understand what happened!
